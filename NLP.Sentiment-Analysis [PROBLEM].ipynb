{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "What is this project about?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Libraries\n",
    "Why are we using these libraries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "05b05ab9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:22:07.517843Z",
     "iopub.status.busy": "2021-12-07T14:22:07.516685Z",
     "iopub.status.idle": "2021-12-07T14:22:09.389728Z",
     "shell.execute_reply": "2021-12-07T14:22:09.388983Z",
     "shell.execute_reply.started": "2021-12-07T13:17:16.522574Z"
    },
    "papermill": {
     "duration": 1.948899,
     "end_time": "2021-12-07T14:22:09.389908",
     "exception": false,
     "start_time": "2021-12-07T14:22:07.441009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Import and Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1cca1117",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:22:10.086920Z",
     "iopub.status.busy": "2021-12-07T14:22:10.086133Z",
     "iopub.status.idle": "2021-12-07T14:22:10.245386Z",
     "shell.execute_reply": "2021-12-07T14:22:10.244778Z",
     "shell.execute_reply.started": "2021-12-07T13:17:18.517214Z"
    },
    "papermill": {
     "duration": 0.233149,
     "end_time": "2021-12-07T14:22:10.245531",
     "exception": false,
     "start_time": "2021-12-07T14:22:10.012382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e61d2cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:22:10.387459Z",
     "iopub.status.busy": "2021-12-07T14:22:10.386765Z",
     "iopub.status.idle": "2021-12-07T14:22:10.413495Z",
     "shell.execute_reply": "2021-12-07T14:22:10.414124Z",
     "shell.execute_reply.started": "2021-12-07T13:17:18.663265Z"
    },
    "papermill": {
     "duration": 0.10019,
     "end_time": "2021-12-07T14:22:10.414298",
     "exception": false,
     "start_time": "2021-12-07T14:22:10.314108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c13b864",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:22:10.558837Z",
     "iopub.status.busy": "2021-12-07T14:22:10.558114Z",
     "iopub.status.idle": "2021-12-07T14:22:10.596904Z",
     "shell.execute_reply": "2021-12-07T14:22:10.597432Z",
     "shell.execute_reply.started": "2021-12-07T13:17:18.694501Z"
    },
    "papermill": {
     "duration": 0.113262,
     "end_time": "2021-12-07T14:22:10.597621",
     "exception": false,
     "start_time": "2021-12-07T14:22:10.484359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f37e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking unique values \n",
    "data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7199e30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking null values in our data\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pre-Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0159bc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert 'tweet_created' to datetime\n",
    "data['tweet_created'] = pd.to_datetime(data['tweet_created']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48a857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc95a8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking uniques values in tweet_created columns\n",
    "data['tweet_created'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e3ca8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "numberoftweets = data.groupby('tweet_created').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71d76c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "numberoftweets.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83a460a",
   "metadata": {},
   "source": [
    "## 2.2 Dealing with Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda451f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ddf2df",
   "metadata": {},
   "source": [
    "[SUGGESTION]: **airline_sentiment_gold, negativereason_gold** have more than 99% missing data And **tweet_coord** have nearly 93% missing data. It will be better to delete these columns as they will not provide any meaningful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3db11b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efc1fe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(['airline_sentiment_gold', 'negativereason_gold','tweet_coord'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95924766",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b4e0c5",
   "metadata": {},
   "source": [
    "## 2.3 What is the biggest reason for having negative comments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5278fa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_reason = data.groupby('negativereason').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fa1f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_reason.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dab8844",
   "metadata": {},
   "source": [
    "# 3. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab63fab1",
   "metadata": {},
   "source": [
    "## 3.1 Type of Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47783a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = data.airline_sentiment.value_counts()\n",
    "index = [1,2,3]\n",
    "plt.bar(index,counter,color=['green','red','blue'])\n",
    "plt.xticks(index,['negative','neutral','positive'],rotation=60)\n",
    "plt.xlabel('Sentiment Type')\n",
    "plt.ylabel('Sentiment Count')\n",
    "plt.title('Count of Type of Sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565a6d23",
   "metadata": {},
   "source": [
    "## 3.2 Airline sentiments for each airline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e5bd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking differtent airlines we have\n",
    "data['airline'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b698b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines= ['US Airways','United','American','Southwest','Delta','Virgin America']\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "for i in airlines:\n",
    "    indices= airlines.index(i)\n",
    "    plt.subplot(2,3,indices+1)\n",
    "    new_df=data[data['airline']==i]\n",
    "    count=new_df['airline_sentiment'].value_counts()\n",
    "    Index = [1,2,3]\n",
    "    plt.bar(Index,count, color=['red', 'green', 'blue'])\n",
    "    plt.xticks(Index,['negative','neutral','positive'])\n",
    "    plt.ylabel('Mood Count')\n",
    "    plt.xlabel('Mood')\n",
    "    plt.title('Count of Moods of '+i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7466c2",
   "metadata": {},
   "source": [
    "## 3.3 Airlines by Negative sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1bf5e232",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_data = data.loc[data['airline_sentiment'] == 'negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de410e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_data.airline.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adc3182",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_data.airline.value_counts().plot.bar()\n",
    "plt.xlabel('Airlines')\n",
    "plt.ylabel('Count of negative tweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8ca12f",
   "metadata": {},
   "source": [
    "## 3.4 Airline by all sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2312aa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "airline_all_sentiment = data.groupby(['airline', 'airline_sentiment']).size()\n",
    "airline_all_sentiment.unstack().plot(kind='bar', stacked=True, figsize=(15,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d62fd96",
   "metadata": {},
   "source": [
    "## 3.5 Is there a relationship between negative sentiments and date?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2046a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958c404e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=negative_data, x='tweet_created', hue='airline')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "364ef9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b06ab890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud,STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7344dbc1",
   "metadata": {},
   "source": [
    "## 3.6 Word cloud of Positive sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1da381",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=data[data['airline_sentiment']=='positive']\n",
    "words = ' '.join(new_df['text'])\n",
    "cleaned_word = \" \".join([word for word in words.split()\n",
    "                            if 'http' not in word\n",
    "                                and not word.startswith('@')\n",
    "                                and word != 'RT'\n",
    "                            ])\n",
    "wordcloud = WordCloud(stopwords=STOPWORDS,\n",
    "                      background_color='black',\n",
    "                      width=3000,\n",
    "                      height=2500\n",
    "                     ).generate(cleaned_word)\n",
    "plt.figure(1,figsize=(12, 12))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf2430d",
   "metadata": {},
   "source": [
    "## 3.7 Word cloud of Negative sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f649dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=data[data['airline_sentiment']=='negative']\n",
    "words = ' '.join(new_df['text'])\n",
    "cleaned_word = \" \".join([word for word in words.split()\n",
    "                            if 'http' not in word\n",
    "                                and not word.startswith('@')\n",
    "                                and word != 'RT'\n",
    "                            ])\n",
    "wordcloud = WordCloud(stopwords=STOPWORDS,\n",
    "                      background_color='black',\n",
    "                      width=3000,\n",
    "                      height=2500\n",
    "                     ).generate(cleaned_word)\n",
    "plt.figure(1,figsize=(12, 12))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67831507",
   "metadata": {},
   "source": [
    "# 4. Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a60a267",
   "metadata": {},
   "source": [
    "### Text Representation\n",
    "- In Natural Language Processing (NLP) the conversion of raw-text to numerical form is called <b>Text Representation</b>\n",
    "- This step is most important in the NLP pipeline because if we feed inappropriate data, our prediction will be useless.\n",
    "\n",
    "If you have good `Text Represntation` and if you use any ordinary algorithm, you will get much better result then if you use highend APIs and algorithm with bad or poor `Text Representation`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5bdd00",
   "metadata": {},
   "source": [
    "### Basic Text Pre-Processing\n",
    "\n",
    "**1. Stop-Word Removal** : In English words like a, an, the, as, in, on, etc. are considered as stop-words so according to our requirements we can remove them to reduce vocabulary size as these words don't have some specific meaning\n",
    "\n",
    "**2. Lower Casing** : Convert all words into the lower case because the upper or lower case may not make a difference for the problem.\n",
    "And we are reducing vocabulary size by doing so. \n",
    "\n",
    "**3. Stemming** : Stemming refers to the process of removing suffixes and reducing a word to some base form such that all different variants of that word can be represented by the same form (e.g., “walk” and “walking” are both reduced to “walk”).\n",
    "\n",
    "**4. Tokenization** : NLP software typically analyzes text by breaking it up into words (tokens) and sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57f93946",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04744cce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:22:11.266128Z",
     "iopub.status.busy": "2021-12-07T14:22:11.265361Z",
     "iopub.status.idle": "2021-12-07T14:22:11.267682Z",
     "shell.execute_reply": "2021-12-07T14:22:11.268298Z",
     "shell.execute_reply.started": "2021-12-07T13:17:18.964893Z"
    },
    "papermill": {
     "duration": 0.080846,
     "end_time": "2021-12-07T14:22:11.268478",
     "exception": false,
     "start_time": "2021-12-07T14:22:11.187632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "def clean_text(d):\n",
    "    pattern = r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, '', d)\n",
    "    return text\n",
    "\n",
    "names = ['delta', 'deltaair', 'united', 'unitedair', 'southwest', 'southwestair', 'usairways',\n",
    "         'virginamerica', 'american', 'americanair', 'jetblue', 'jetblues', 'usairway',\n",
    "         'flight', 'airline', 'airlines']\n",
    "\n",
    "# the words related the name of airlines are not relevant to the sentiment analysis\n",
    "# Therefore I decided to append the above names into the list of stop words.\n",
    "# you can also append any names in to the list for custom cleaning.\n",
    "\n",
    "def clean_stopword(d):\n",
    "    stop_words = stopwords.words('english')\n",
    "    for name in names:\n",
    "        stop_words.append(name)\n",
    "    return \" \".join([w.lower() for w in d.split() if w.lower() not in stop_words and len(w) > 1])\n",
    "\n",
    "def tokenize(d):\n",
    "    return word_tokenize(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ae080b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:22:11.418865Z",
     "iopub.status.busy": "2021-12-07T14:22:11.418160Z",
     "iopub.status.idle": "2021-12-07T14:22:11.421007Z",
     "shell.execute_reply": "2021-12-07T14:22:11.421498Z",
     "shell.execute_reply.started": "2021-12-07T13:17:18.972057Z"
    },
    "papermill": {
     "duration": 0.082756,
     "end_time": "2021-12-07T14:22:11.421681",
     "exception": false,
     "start_time": "2021-12-07T14:22:11.338925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['final_text']= df.text.apply(clean_text).apply(clean_stopword).apply(tokenize)\n",
    "df.final_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f807e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T02:01:42.239808Z",
     "iopub.status.busy": "2022-02-14T02:01:42.239090Z",
     "iopub.status.idle": "2022-02-14T02:01:42.248396Z",
     "shell.execute_reply": "2022-02-14T02:01:42.246703Z",
     "shell.execute_reply.started": "2022-02-14T02:01:42.239766Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\" \".join(df.final_text[0]),'\\n')\n",
    "print(\" \".join(df.final_text[1]),'\\n')\n",
    "print(\" \".join(df.final_text[100]),'\\n')\n",
    "print(\" \".join(df.final_text[1000]),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501cde6e",
   "metadata": {},
   "source": [
    "# 5. Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e090b4",
   "metadata": {},
   "source": [
    "## 5.1 Lexicon based approach\n",
    "\n",
    "The lexicon-based approach involves calculating orientation for a document from the semantic orientation of words or phrases in the document.it uses dictionaries of words annotated with the word's semantic orientation, or polarity.\n",
    "\n",
    "Here we will just try to verify the lexicon works hopefully very well to classify the sentiment which has already been allocated by the real customers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9aad2c5",
   "metadata": {},
   "source": [
    "### 5.1.1 Vader\n",
    "\n",
    "Vader is a lexicon made for the sentiment analysis of text data on social media.\n",
    "\n",
    "It returns the compounded polarity score of a sentence after calculating the negative, the neutral, and the positive scores of each word in a sentence.\n",
    "\n",
    "We can use Vader from the innner module in nltk as well as the Python API [vaderSentiment](https://pypi.org/project/vaderSentiment/).\n",
    "\n",
    "VADER has many advantages over traditional sentiment analysis methods: [source](https://ichi.pro/ko/python-eseo-vaderleul-sayonghayeo-gamjeong-bunseog-dansunhwa-sosyeol-midieo-tegseuteu-274770204542255)\n",
    "\n",
    "* **It works very well with social media type text**, but generalizes easily to multiple domains.\n",
    "* It does not require any training data, but consists of a generalized, balence-based, human-curated gold standard psychological vocabulary.\n",
    "* It's fast enough to use online with streaming data,\n",
    "* The speed-performance trade-off isn't serious.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c75ece7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T02:01:52.697985Z",
     "iopub.status.busy": "2022-02-14T02:01:52.697526Z",
     "iopub.status.idle": "2022-02-14T02:01:52.728888Z",
     "shell.execute_reply": "2022-02-14T02:01:52.728216Z",
     "shell.execute_reply.started": "2022-02-14T02:01:52.697947Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "vader.polarity_scores(\" \".join(df.final_text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6351bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T02:02:23.150110Z",
     "iopub.status.busy": "2022-02-14T02:02:23.149363Z",
     "iopub.status.idle": "2022-02-14T02:02:23.322312Z",
     "shell.execute_reply": "2022-02-14T02:02:23.321596Z",
     "shell.execute_reply.started": "2022-02-14T02:02:23.150048Z"
    }
   },
   "outputs": [],
   "source": [
    "texts = [\" \".join(df.final_text[i]) for i in range(len(df))]\n",
    "\n",
    "print(df.text[0])\n",
    "print(texts[0])\n",
    "print(vader.polarity_scores(texts[0]), f'--> Actual Classification: {df.airline_sentiment[0]}', '\\n')\n",
    "\n",
    "print(df.text[1])\n",
    "print(texts[1])\n",
    "print(vader.polarity_scores(texts[1]), f'--> Actual Classification: {df.airline_sentiment[1]}', '\\n')\n",
    "\n",
    "print(df.text[10])\n",
    "print(texts[10])\n",
    "print(vader.polarity_scores(texts[10]), f'--> Actual Classification: {df.airline_sentiment[10]}', '\\n')\n",
    "\n",
    "print(df.text[100])\n",
    "print(texts[100])\n",
    "print(vader.polarity_scores(texts[100]), f'--> Actual Classification: {df.airline_sentiment[100]}', '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531d590e",
   "metadata": {},
   "source": [
    "## 5.2 Machine Learning approach\n",
    "\n",
    "ML approach is also known as Document Classificaton. It uses ML/DL algorithm to classify the text data.\n",
    "\n",
    "The most important in ML approach is to convert the text to the vector or other numeric format to make the algorithms understand the text as a feature.\n",
    "\n",
    "\n",
    "* Vectorization : `CountVectorizer`, `TfidfTransformer`, `Word2Vec`\n",
    "* Model: Logistic Regression, RandomForest, SupportVectorMachine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d2bb9e06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T05:24:43.402309Z",
     "iopub.status.busy": "2022-02-14T05:24:43.402035Z",
     "iopub.status.idle": "2022-02-14T05:24:43.495016Z",
     "shell.execute_reply": "2022-02-14T05:24:43.494221Z",
     "shell.execute_reply.started": "2022-02-14T05:24:43.402279Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd4ca55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T05:17:25.225146Z",
     "iopub.status.busy": "2022-02-14T05:17:25.224842Z",
     "iopub.status.idle": "2022-02-14T05:17:25.234860Z",
     "shell.execute_reply": "2022-02-14T05:17:25.234170Z",
     "shell.execute_reply.started": "2022-02-14T05:17:25.225111Z"
    }
   },
   "outputs": [],
   "source": [
    "df.final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e4a2f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T05:17:28.301131Z",
     "iopub.status.busy": "2022-02-14T05:17:28.300464Z",
     "iopub.status.idle": "2022-02-14T05:17:33.623760Z",
     "shell.execute_reply": "2022-02-14T05:17:33.623063Z",
     "shell.execute_reply.started": "2022-02-14T05:17:28.301095Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    df['final_text'][i] = \" \".join(df['final_text'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d606d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T05:17:35.107705Z",
     "iopub.status.busy": "2022-02-14T05:17:35.107031Z",
     "iopub.status.idle": "2022-02-14T05:17:35.113954Z",
     "shell.execute_reply": "2022-02-14T05:17:35.113300Z",
     "shell.execute_reply.started": "2022-02-14T05:17:35.107661Z"
    }
   },
   "outputs": [],
   "source": [
    "df['final_text'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fa782454",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T05:17:39.039451Z",
     "iopub.status.busy": "2022-02-14T05:17:39.038608Z",
     "iopub.status.idle": "2022-02-14T05:17:39.059387Z",
     "shell.execute_reply": "2022-02-14T05:17:39.058677Z",
     "shell.execute_reply.started": "2022-02-14T05:17:39.039403Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_Sentiment(sentiment):\n",
    "    if  sentiment == \"positive\":\n",
    "        return 2\n",
    "    elif sentiment == \"neutral\":\n",
    "        return 1\n",
    "    elif sentiment == \"negative\":\n",
    "        return 0\n",
    "    \n",
    "df.airline_sentiment = df.airline_sentiment.apply(lambda x : convert_Sentiment(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "14c0ba83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T05:17:39.972099Z",
     "iopub.status.busy": "2022-02-14T05:17:39.971506Z",
     "iopub.status.idle": "2022-02-14T05:17:39.976162Z",
     "shell.execute_reply": "2022-02-14T05:17:39.975389Z",
     "shell.execute_reply.started": "2022-02-14T05:17:39.972061Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df.final_text\n",
    "y = df.airline_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76afae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef10d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461fe37f",
   "metadata": {},
   "source": [
    "### 5.2.1 CountVectorizer & TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0b006f44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T02:14:26.001561Z",
     "iopub.status.busy": "2022-02-14T02:14:26.001290Z",
     "iopub.status.idle": "2022-02-14T02:20:59.144794Z",
     "shell.execute_reply": "2022-02-14T02:20:59.144103Z",
     "shell.execute_reply.started": "2022-02-14T02:14:26.001528Z"
    }
   },
   "outputs": [],
   "source": [
    "# model_params = {'random_state':42}\n",
    "# model_list = [LogisticRegression(**model_params, solver='liblinear'),\n",
    "#               RandomForestClassifier(**model_params),\n",
    "#               SVC(**model_params)]\n",
    "# model_names = ['LogisticRegression', 'RandomForest', 'SupportVectorMachine']\n",
    "\n",
    "# c = CountVectorizer()\n",
    "# X_c = c.fit_transform(X)\n",
    "\n",
    "# tf = TfidfVectorizer()\n",
    "# X_tf = tf.fit_transform(X)\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# for model, model_name in zip(model_list, model_names):\n",
    "#     for n_fold, (trn_idx, vld_idx) in enumerate(skf.split(df.index, df.airline_sentiment)):        \n",
    "#         X_c_trn = X_c[trn_idx]\n",
    "#         X_tf_trn = X_tf[trn_idx]\n",
    "#         y_trn = df.loc[trn_idx, 'airline_sentiment']\n",
    "        \n",
    "#         X_c_vld = X_c[vld_idx]\n",
    "#         X_tf_vld = X_tf[vld_idx]\n",
    "#         y_vld = df.loc[vld_idx, 'airline_sentiment']        \n",
    "        \n",
    "#         model.fit(X_c_trn, y_trn)\n",
    "#         c_pred_col = f\"{model_name}_ct_pred\"\n",
    "#         df.loc[vld_idx, c_pred_col] = model.predict(X_c_vld)\n",
    "        \n",
    "#         model.fit(X_tf_trn, y_trn)\n",
    "#         tf_pred_col = f\"{model_name}_tf_pred\"\n",
    "#         df.loc[vld_idx, tf_pred_col] = model.predict(X_tf_vld)\n",
    "\n",
    "#     print(f\"Model: {model_name}, CountVectorizer, Accuracy: {accuracy_score(df.airline_sentiment, df[c_pred_col]):.3%}\")\n",
    "#     print(f\"Model: {model_name}, TfidfVectorizer, Accuracy: {accuracy_score(df.airline_sentiment, df[tf_pred_col]):.3%}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0782af",
   "metadata": {},
   "source": [
    "### 5.2.2 Word2Vec \n",
    "\n",
    "Word2Vec is one of the most popular model to represent a word in a large text corpus as a vector in n-dimensional space.\n",
    "\n",
    "There are two kinds of W2V, Continuous Bag-of-Words(CBOW) and Skip-Gram.\n",
    "\n",
    "Skip-gram is used to predict the context word for a given target word. It’s reverse of CBOW algorithm. Here, target word is input while context words are output.\n",
    "\n",
    "In most case it is known that the predictability of skip-gram is better than the one of CBOW.\n",
    "\n",
    "We can use `Word2Vec` library from `gensim` and set the option `sg` which is the abbreviation of skip-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "30ab3fe7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T02:22:53.097009Z",
     "iopub.status.busy": "2022-02-14T02:22:53.096362Z",
     "iopub.status.idle": "2022-02-14T02:22:56.610154Z",
     "shell.execute_reply": "2022-02-14T02:22:56.609415Z",
     "shell.execute_reply.started": "2022-02-14T02:22:53.096970Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "texts_w2v = df.final_text.apply(tokenize).to_list()\n",
    "\n",
    "w2v = Word2Vec(sentences = texts_w2v, window = 3,\n",
    "               vector_size = 100, min_count = 5, workers = 4, sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48aae04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T02:23:17.595578Z",
     "iopub.status.busy": "2022-02-14T02:23:17.595324Z",
     "iopub.status.idle": "2022-02-14T02:23:17.601208Z",
     "shell.execute_reply": "2022-02-14T02:23:17.600373Z",
     "shell.execute_reply.started": "2022-02-14T02:23:17.595550Z"
    }
   },
   "outputs": [],
   "source": [
    "texts_w2v[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f49aa33",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "`Word2Vec` imported from `gensim` trains the texts at the same time as it is declared by the user. \n",
    "\n",
    "We can find the similar words with the given word and the examples are represented below.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84e1e08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T02:23:58.189897Z",
     "iopub.status.busy": "2022-02-14T02:23:58.189618Z",
     "iopub.status.idle": "2022-02-14T02:23:58.200811Z",
     "shell.execute_reply": "2022-02-14T02:23:58.199803Z",
     "shell.execute_reply.started": "2022-02-14T02:23:58.189867Z"
    }
   },
   "outputs": [],
   "source": [
    "w2v.wv.most_similar('thank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d053a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T02:24:00.517679Z",
     "iopub.status.busy": "2022-02-14T02:24:00.517001Z",
     "iopub.status.idle": "2022-02-14T02:24:00.530294Z",
     "shell.execute_reply": "2022-02-14T02:24:00.528892Z",
     "shell.execute_reply.started": "2022-02-14T02:24:00.517643Z"
    }
   },
   "outputs": [],
   "source": [
    "w2v.wv.most_similar('customerservice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Conclusions - Suggestions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 853.142536,
   "end_time": "2021-12-07T14:36:10.246332",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-12-07T14:21:57.103796",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
